# ETL Pipeline для обработки данных DLE комиксов

Этот проект содержит ETL пайплайн для автоматизации извлечения, очистки и нормализации данных из CSV экспортов DataLife Engine (DLE) в набор готовых к импорту CSV файлов.

## Структура проекта

```
.
├── etl/                      # Модули ETL пайплайна
│   ├── extract/             # Загрузка исходных данных
│   ├── transform/           # Преобразование данных
│   ├── load/                # Сохранение результатов
│   ├── utils/               # Вспомогательные функции
│   ├── config.py            # Конфигурация
│   ├── pipeline.py          # Основной пайплайн
│   └── quality.py           # Валидация качества
├── data/
│   ├── raw/                 # Исходные данные (не используется, CSV в корне)
│   └── processed/           # Результаты обработки
├── logs/                     # Логи выполнения и отчёты
├── docs/                     # Документация
│   └── target_schema.md     # Описание целевой схемы
├── run_etl.py               # Точка входа
└── requirements.txt         # Зависимости Python
```

## Установка

### Требования

- Python 3.8 или выше
- pip

### Установка зависимостей

```bash
pip install -r requirements.txt
```

Или с использованием виртуального окружения:

```bash
python3 -m venv venv
source venv/bin/activate  # На Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Запуск

### Базовый запуск

```bash
python run_etl.py
```

Или:

```bash
./run_etl.py
```

### Что происходит при запуске

1. **Загрузка данных (Extract)**: Читает все исходные CSV файлы DLE из корневой директории проекта
2. **Преобразование (Transform)**: 
   - Категории → Издатели и Серии
   - Посты → Выпуски комиксов
   - Xfields/xfsearch → Участники и роли
   - Теги и ссылки на скачивание
3. **Сохранение (Load)**: Записывает результаты в `data/processed/`
4. **Валидация качества**: Генерирует отчёт в `logs/etl_report.md`

## Входные данные

Пайплайн ожидает следующие CSV файлы в корне проекта:

- `dle_post_202510281639.csv` - основные посты/статьи (выпуски комиксов)
- `dle_category_202510281639.csv` - категории (издатели и серии)
- `dle_post_extras_202510281639.csv` - дополнительные метаданные постов
- `dle_post_extras_cats_202510281639.csv` - связи постов и категорий
- `dle_xfsearch_202510281639.csv` - индексированные пользовательские поля
- `dle_users_202510281639.csv` - пользователи
- `dle_usergroups_202510281639.csv` - группы пользователей
- `dle_tags_202510281639.csv` - теги

## Выходные данные

Результаты обработки сохраняются в `data/processed/`:

- `publishers.csv` - издатели комиксов (Marvel, DC, etc.)
- `series.csv` - серии комиксов
- `comic_issues.csv` - отдельные выпуски комиксов
- `contributors.csv` - участники (переводчики, дизайнеры, редакторы)
- `roles.csv` - словарь ролей
- `issue_contributors.csv` - связи выпусков и участников
- `issue_tags.csv` - теги выпусков
- `issue_downloads.csv` - ссылки на скачивание и онлайн-ридер

Подробное описание схемы см. в [docs/target_schema.md](docs/target_schema.md)

## Основные возможности

### Очистка HTML

- Удаление HTML тегов из описаний
- Извлечение чистого текста
- Извлечение URL первого изображения (обложки)

### Обработка xfields

Пайплайн парсит структурированные данные из поля `xfields`:

```
volume|Том 4||perevodchik|vantus||oformlenie|jimjack||taiper|Overlord||redaktor|Валя, rurumiya||download|https://...||reader|
```

Извлекаются:
- Том/выпуск
- Переводчики
- Дизайнеры/оформители
- Наборщики текста
- Редакторы
- Ссылки на скачивание
- Ссылки на онлайн-ридер

### Нормализация участников

- Удаление лишних пробелов
- Обработка списков (через запятую или слэш)
- Сопоставление с пользователями DLE по имени/email
- Отслеживание несопоставленных участников

### Иерархия категорий

- **Издатели**: категории верхнего уровня (parentid=0)
- **Серии**: подкатегории (parentid>0)
- Поддержка многоуровневой вложенности

## Логи и отчёты

### Логи выполнения

Все логи сохраняются в `logs/etl_YYYYMMDD_HHMMSS.log`

### Отчёт о качестве

После выполнения генерируется отчёт `logs/etl_report.md`, содержащий:

- Статистику по количеству записей
- Ошибки валидации
- Предупреждения о потерях данных
- Список несопоставленных участников
- Проверку ссылочной целостности

## Конфигурация

Основные настройки находятся в `etl/config.py`:

- Пути к файлам
- Маппинг ролей (русский ↔ английский)
- Параметры обработки HTML
- Пороги валидации качества

## Устранение неполадок

### Проблема: "File not found"

Убедитесь, что CSV файлы находятся в корне проекта и имеют правильные имена.

### Проблема: Ошибки кодировки

Все файлы обрабатываются как UTF-8. Если у вас другая кодировка, измените `CSV_ENCODING` в `etl/config.py`.

### Проблема: Пустые результаты

Проверьте логи и отчёт о качестве для деталей. Возможные причины:
- Некорректная структура исходных CSV
- Отсутствие обязательных полей
- Проблемы с кодировкой

## Разработка

### Добавление новой роли

Отредактируйте `ROLE_MAPPINGS` в `etl/config.py`:

```python
ROLE_MAPPINGS = {
    'new_role': {
        'name_en': 'new_role_en', 
        'description': 'Описание роли'
    },
    ...
}
```

### Добавление новой проверки качества

Добавьте метод в класс `QualityReport` в `etl/quality.py`.

## Лицензия

Этот проект создан для внутреннего использования.
